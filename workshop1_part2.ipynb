{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMATH 50 Workshop 1: Optimization\n",
    "**Part II**\n",
    "\n",
    "In this coding exercise, we try to program up\n",
    "\n",
    "* Proximal Gradient Descent\n",
    "\n",
    "algorithms use logistic regression as an example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_images = np.load('data/train_images.npy')\n",
    "train_labels = np.load('data/train_labels.npy')\n",
    "test_images = np.load('data/test_images.npy')\n",
    "test_labels = np.load('data/test_labels.npy')\n",
    "\n",
    "num_train_images = train_labels.size\n",
    "num_test_images = test_labels.size\n",
    "image_size = 28*28\n",
    "\n",
    "print('number of training images: %i' % num_train_images)\n",
    "print('number of testing images: %i' % num_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Gradient Descent\n",
    "\n",
    "Now let us consider the strutural non-smooth optimization problem,\n",
    "$$\n",
    "\\min_x f(x) := h(x) + g(x)\n",
    "$$\n",
    "where $h$ is a $\\beta$-smooth convex function and $g$ is convex but not necessarily smooth.\n",
    "\n",
    "The proximal gradient method can be summaried as follows,\n",
    "\n",
    "* input: $x_0$\n",
    "* set $k = 0$\n",
    "* while $\\beta\\|x_k - x_{k-1}\\| \\ge $ tolerance\n",
    "\\begin{align*}\n",
    "x_{k+1} &\\leftarrow \\text{prox}_{g/\\beta}\\left(x_k - \\frac{1}{\\beta} \\nabla h(x_k)\\right)\\\\\n",
    "k &\\leftarrow k + 1\n",
    "\\end{align*}\n",
    "end\n",
    "* output: $x_k$\n",
    "\n",
    "Now let us consider the logistic regression with $\\ell_1$ norm regularizer,\n",
    "$$\n",
    "h(x) = \\sum_{i=1}^m [\\ln(1 + \\exp(\\langle a_i, x \\rangle)) - y_i\\langle a_i, x \\rangle], \\quad\n",
    "g(x) = \\lambda \\|x\\|_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "A = train_images\n",
    "y = train_labels\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "beta_h = 0.25*np.linalg.norm(A, 2)**2\n",
    "\n",
    "def func_h(x):\n",
    "    z = A.dot(x)\n",
    "    return np.sum(np.log(1.0 + np.exp(z)) - y*z)\n",
    "\n",
    "def grad_h(x):\n",
    "    z = A.dot(x)\n",
    "    return A.T.dot(np.exp(z)/(1.0 + np.exp(z)) - y)\n",
    "\n",
    "def func_g(x):\n",
    "    return lam*np.sum(np.abs(x))\n",
    "\n",
    "def prox_g(x, t):\n",
    "    t = lam*t\n",
    "    y = np.zeros(x.size, dtype=x.dtype)\n",
    "\n",
    "    ind = np.where(np.abs(x) > t)\n",
    "    x_o = x[ind]\n",
    "\n",
    "    y[ind] = np.sign(x_o)*(np.abs(x_o) - t)\n",
    "\n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal gradient descent\n",
    "def optimizeWithPGD(x0, func_h, func_g, grad_h, prox_g, beta_h, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Optimize with Proximal Gradient Descent Method\n",
    "        min_x h(x) + g(x)\n",
    "    where h is beta smooth and g is proxiable.\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    x0 : array_like\n",
    "        Starting point for the solver\n",
    "    func_h : function\n",
    "        Input x and return the function value of h\n",
    "    func_g : function\n",
    "        Input x and return the function value of g\n",
    "    grad_h : function\n",
    "        Input x and return the gradient of h\n",
    "    prox_g : function\n",
    "        Input x and a constant float number and return the prox solution\n",
    "    beta_h : float\n",
    "        beta smoothness constant for h\n",
    "    tol : float, optional\n",
    "        Gradient tolerance for terminating the solver.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iteration for terminating the solver.\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    x : array_like\n",
    "        Final solution\n",
    "    obj_his : array_like\n",
    "        Objective function value convergence history\n",
    "    err_his : array_like\n",
    "        Norm of gradient convergence history\n",
    "    exit_flag : int\n",
    "        0, norm of gradient below `tol`\n",
    "        1, exceed maximum number of iteration\n",
    "        2, others\n",
    "    \"\"\"\n",
    "    # initial information\n",
    "    x = x0.copy()\n",
    "    g = grad_h(x)\n",
    "    #\n",
    "    step_size = 1.0/beta_h\n",
    "    # not recording the initial point since we do not have measure of the optimality\n",
    "    obj_his = np.zeros(max_iter)\n",
    "    err_his = np.zeros(max_iter)\n",
    "    \n",
    "    # start iteration\n",
    "    iter_count = 0\n",
    "    err = tol + 1.0\n",
    "    while err >= tol:\n",
    "        # proximal gradient descent step\n",
    "        x_new = prox_g(x - step_size*g, step_size)\n",
    "        #\n",
    "        # update information\n",
    "        obj = func_h(x_new) + func_g(x_new)\n",
    "        err = np.linalg.norm(x - x_new)/step_size\n",
    "        #\n",
    "        np.copyto(x, x_new)\n",
    "        g = grad_h(x)\n",
    "        #\n",
    "        obj_his[iter_count] = obj\n",
    "        err_his[iter_count] = err\n",
    "        #\n",
    "        # check if exceed maximum number of iteration\n",
    "        iter_count += 1\n",
    "        if iter_count >= max_iter:\n",
    "            print('Proximal gradient descent reach maximum of iteration')\n",
    "            return x, obj_his[:iter_count], err_his[:iter_count], 1\n",
    "    #\n",
    "    return x, obj_his[:iter_count], err_his[:iter_count], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the algorithm\n",
    "x0 = np.zeros(image_size)\n",
    "\n",
    "x, obj_his, err_his, exit_flag = optimizeWithPGD(x0, func_h, func_g, grad_h, prox_g, beta_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot convergence result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his)\n",
    "ax[1].set_title('optimality condition')\n",
    "fig.suptitle('Proximal Gradient Descent on Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the identifier\n",
    "plt.imshow(x.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test result\n",
    "A_test = test_images\n",
    "y_test = test_labels\n",
    "\n",
    "z_test = A_test.dot(x)\n",
    "\n",
    "rate = np.sum(((z_test < 0.0) & (y_test == 0.0)) | ((z_test > 0.0) & (y_test == 1.0)))/num_test_images\n",
    "\n",
    "print('correctly classify %0.2f of the testing data.' % rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
