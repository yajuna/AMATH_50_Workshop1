{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMATH 50 Workshop 1: Optimization\n",
    "**Part III**\n",
    "\n",
    "In this coding exercise, we try to program up\n",
    "\n",
    "* PALM Algorithm for Robust Logistic Regression\n",
    "* SR3 Algorithm for Fast Sparse Logistic Regression\n",
    "\n",
    "algorithms use logistic regression as an example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_images = np.load('data/train_images.npy')\n",
    "train_labels = np.load('data/train_labels.npy')\n",
    "test_images = np.load('data/test_images.npy')\n",
    "test_labels = np.load('data/test_labels.npy')\n",
    "\n",
    "num_train_images = train_labels.size\n",
    "num_test_images = test_labels.size\n",
    "image_size = 28*28\n",
    "\n",
    "print('number of training images: %i' % num_train_images)\n",
    "print('number of testing images: %i' % num_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PALM Algorithm\n",
    "\n",
    "[PALM](https://pdfs.semanticscholar.org/c2a3/59e9408191c9b26dfb7d4fc5389da19e208d.pdf)\n",
    "algorithm is one of the most flexible method that can deal with structural non-smooth non-convex objective.\n",
    "And [Trimming](https://arxiv.org/abs/1610.01101) is a very powerful robustification of any summable objective.\n",
    "In this part, we will see how can we apply PALM to solve trimmed logistic regression.\n",
    "\n",
    "Consider optimization problem,\n",
    "$$\n",
    "\\min_{x, w\\in\\triangle_k}~~f(x,w):=\\sum_{i=1}^m w_i h_i(x) + g(x)\n",
    "$$\n",
    "where,\n",
    "$$\n",
    "h_i(x) = \\ln(1 + \\exp(\\langle a_i, x \\rangle)) - y_i \\langle a_i, x \\rangle, \\quad\n",
    "g(x) = \\lambda \\|x\\|_1,\n",
    "$$\n",
    "and $\\triangle_k$ is the $k$-capped simplex,\n",
    "$$\n",
    "\\triangle_k = \\left\\{w : w\\in [0,1]^m, \\sum_{i=1}^m w_i = k\\right\\}.\n",
    "$$\n",
    "\n",
    "PALM algorithm for this objective can be summarized as follows,\n",
    "\n",
    "* input: $x_0$, $w_0$, $\\tau$\n",
    "* set $k = 0$\n",
    "* while $\\beta\\|x_k - x_{k-1}\\| \\ge $ tolerance\n",
    "\\begin{align*}\n",
    "x_{k+1} &\\leftarrow \\text{prox}_{g/\\beta}(x_k - \\tfrac{1}{\\beta} \\nabla_x f(x_k, w_k))\\\\\n",
    "w_{k+1} &\\leftarrow \\text{proj}_{\\triangle_h}(w_k - \\tau \\nabla_w f(x_{k+1}, w_k))\\\\\n",
    "k &\\leftarrow k + 1\n",
    "\\end{align*}\n",
    "end\n",
    "* output: $x_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "A = train_images\n",
    "y = train_labels\n",
    "\n",
    "lam = 1.0\n",
    "\n",
    "beta_h = 0.25*np.linalg.norm(A, 2)**2\n",
    "\n",
    "def func_h(x):\n",
    "    z = A.dot(x)\n",
    "    return np.log(1.0 + np.exp(z)) - y*z\n",
    "\n",
    "def grad_h(x, w):\n",
    "    z = A.dot(x)\n",
    "    return (A.T*w).dot(np.exp(z)/(1.0 + np.exp(z)) - y)\n",
    "\n",
    "def func_g(x):\n",
    "    return lam*np.sum(np.abs(x))\n",
    "\n",
    "def prox_g(x, t):\n",
    "    t = lam*t\n",
    "    y = np.zeros(x.size, dtype=x.dtype)\n",
    "\n",
    "    ind = np.where(np.abs(x) > t)\n",
    "    x_o = x[ind]\n",
    "\n",
    "    y[ind] = np.sign(x_o)*(np.abs(x_o) - t)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal gradient descent\n",
    "def optimizeWithTrimmingPGD(x0, func_h, func_g, grad_h, prox_g, beta_h, num_inliers, N,\n",
    "                            weight_step_size=1.0, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Optimize with Trimming Proximal Gradient Descent Method\n",
    "        min_{x, w} <w, h(x)> + g(x)\n",
    "    where h is beta smooth and g is proxiable.\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    x0 : array_like\n",
    "        Starting point for the solver\n",
    "    func_h : function\n",
    "        Input x and return the function value of h\n",
    "    func_g : function\n",
    "        Input x and return the function value of g\n",
    "    grad_h : function\n",
    "        Input x and return the gradient of h\n",
    "    prox_g : function\n",
    "        Input x and a constant float number and return the prox solution\n",
    "    beta_h : float\n",
    "        beta smoothness constant for h\n",
    "    num_inliers : int\n",
    "        number of inliers\n",
    "    N : int\n",
    "        total number of data points\n",
    "    weight_step_size : float, optional\n",
    "        step size for the trimming weight\n",
    "    tol : float, optional\n",
    "        Gradient tolerance for terminating the solver.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iteration for terminating the solver.\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    x : array_like\n",
    "        Final solution\n",
    "    w : array_like\n",
    "        weights of the data points\n",
    "    obj_his : array_like\n",
    "        Objective function value convergence history\n",
    "    err_his : array_like\n",
    "        Norm of gradient convergence history\n",
    "    exit_flag : int\n",
    "        0, norm of gradient below `tol`\n",
    "        1, exceed maximum number of iteration\n",
    "        2, others\n",
    "    \"\"\"\n",
    "    # initial information\n",
    "    x = x0.copy()\n",
    "    w = np.repeat(num_inliers/N, N)\n",
    "    g = grad_h(x, w)\n",
    "    #\n",
    "    step_size = 1.0/beta_h\n",
    "    # not recording the initial point since we do not have measure of the optimality\n",
    "    obj_his = np.zeros(max_iter)\n",
    "    err_his = np.zeros(max_iter)\n",
    "    \n",
    "    # start iteration\n",
    "    iter_count = 0\n",
    "    err = tol + 1.0\n",
    "    while err >= tol:\n",
    "        # proximal gradient descent step\n",
    "        x_new = prox_g(x - step_size*g, step_size)\n",
    "        w_new = projCappedSimplex(w - weight_step_size*func_h(x_new), num_inliers)\n",
    "        \n",
    "        # update information\n",
    "        obj = w_new.dot(func_h(x_new)) + func_g(x_new)\n",
    "        err = np.linalg.norm(x - x_new)/step_size\n",
    "        #\n",
    "        np.copyto(x, x_new)\n",
    "        np.copyto(w, w_new)\n",
    "        g = grad_h(x, w)\n",
    "        #\n",
    "        obj_his[iter_count] = obj\n",
    "        err_his[iter_count] = err\n",
    "        #\n",
    "        # check if exceed maximum number of iteration\n",
    "        iter_count += 1\n",
    "        if iter_count >= max_iter:\n",
    "            print('Proximal gradient descent with trimming reach maximum of iteration')\n",
    "            return x, w, obj_his[:iter_count], err_his[:iter_count], 1\n",
    "    #\n",
    "    return x, w, obj_his[:iter_count], err_his[:iter_count], 0\n",
    "\n",
    "def projCappedSimplex(w, w_sum):\n",
    "    a = np.min(w) - 1.0\n",
    "    b = np.max(w) - 0.0\n",
    "\n",
    "    def f(x):\n",
    "        return np.sum(np.maximum(np.minimum(w - x, 1.0), 0.0)) - w_sum\n",
    "\n",
    "    x = bisect(f, a, b)\n",
    "\n",
    "    return np.maximum(np.minimum(w - x, 1.0), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the algorithm\n",
    "x0 = np.zeros(image_size)\n",
    "\n",
    "x, w, obj_his, err_his, exit_flag = optimizeWithTrimmingPGD(x0, func_h, func_g, grad_h, prox_g, beta_h,\n",
    "                                                            int(0.95*num_train_images), num_train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot convergence result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his)\n",
    "ax[1].set_title('optimality condition')\n",
    "fig.suptitle('Proximal Gradient Descent on Logistic Regression With Trimming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the identifier\n",
    "plt.imshow(x.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test result\n",
    "A_test = test_images\n",
    "y_test = test_labels\n",
    "\n",
    "z_test = A_test.dot(x)\n",
    "\n",
    "rate = np.sum(((z_test < 0.0) & (y_test == 0.0)) | ((z_test > 0.0) & (y_test == 1.0)))/num_test_images\n",
    "\n",
    "print('correctly classify %0.2f of the testing data.' % rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
