{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMATH 50 Workshop 1: Optimization\n",
    "**Part I**\n",
    "\n",
    "In this coding exercise, we try to program up\n",
    "\n",
    "* Gradient Descent\n",
    "\n",
    "algorithms use logistic regression as an example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is one of the most fundamental method in machine learning.\n",
    "Consider data set $\\{a_i, y_i\\}_{i=1}^m$, where $a_i \\in \\mathbb{R}^n$ is the data point and\n",
    "$y_i \\in \\{0, 1\\}$ is the corresponding label.\n",
    "\n",
    "Our goal is to train a classifer $x \\in \\mathbb{R}^n$, such that the sign of $\\langle a_i,x \\rangle$ will indicate which class of $a_i$ belongs to, 0 or 1.\n",
    "In order to do that following optimization problem are often considered,\n",
    "$$\n",
    "\\min_x ~~\\sum_{i=1}^m [\\ln(1 + \\exp(\\langle a_i, x \\rangle)) - y_i\\langle a_i, x \\rangle].\n",
    "$$\n",
    "And we will consider different variations of it though the workshop.\n",
    "For more details of the derivation of the objective please check the lecture notes.\n",
    "\n",
    "**Data Set**: We use MNIST 0 and 1 digits as the example data set, for more details please check [MNIST](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_images = np.load('data/train_images.npy')\n",
    "train_labels = np.load('data/train_labels.npy')\n",
    "test_images = np.load('data/test_images.npy')\n",
    "test_labels = np.load('data/test_labels.npy')\n",
    "\n",
    "num_train_images = train_labels.size\n",
    "num_test_images = test_labels.size\n",
    "image_size = 28*28\n",
    "\n",
    "print('number of training images: %i' % num_train_images)\n",
    "print('number of testing images: %i' % num_test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[2].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Recall that if we consider a $\\beta$-smooth objective $f$, and want to solve the optimizatoin problem,\n",
    "$$\n",
    "\\min_{x}~~f(x)\n",
    "$$\n",
    "the gradient descend algorithm can be specify as,\n",
    "\n",
    "* input: $x_0$\n",
    "* set $k = 0$\n",
    "* while $\\|\\nabla f(x_k)\\| \\ge $ tolerance\n",
    "\\begin{align*}\n",
    "x_{k+1} &\\leftarrow x_k - \\frac{1}{\\beta} \\nabla f(x_k)\\\\\n",
    "k &\\leftarrow k + 1\n",
    "\\end{align*}\n",
    "end\n",
    "* output: $x_k$ \n",
    "\n",
    "Now let us consider the algorithm in the context of logistic regression, where,\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^m [\\ln(1 + \\exp(\\langle a_i, x \\rangle)) - y_i\\langle a_i, x \\rangle] + \\frac{\\lambda}{2} \\|x\\|^2.\n",
    "$$\n",
    "\n",
    "Moreover,\n",
    "$$\n",
    "\\nabla f(x) = \\sum_{i=1}^m \\frac{\\exp(\\langle a_i, x \\rangle)}{1 + \\exp(\\langle a_i, x \\rangle)} a_i - y_i a_i + \\lambda x, \\quad\n",
    "\\nabla^2 f(x) = \\sum_{i=1}^m \\frac{\\exp(\\langle a_i, x \\rangle)}{(1+\\exp(\\langle a_i, x \\rangle))^2} a_i a_i^\\top + \\lambda I.\n",
    "$$\n",
    "\n",
    "And since\n",
    "$$\n",
    "\\frac{z}{(1+z)^2} \\le \\frac{1}{4}, \\quad \\forall z \\ge 0,\n",
    "$$\n",
    "we have,\n",
    "$$\n",
    "\\|\\nabla^2 f(x)\\| \\le \\frac{1}{4}\\|A\\|^2 + \\lambda\n",
    "$$\n",
    "where $A = [a_1, a_2, \\ldots, a_m]$. Therefore we could set our $\\beta = \\frac{1}{4}\\|A\\|^2 + \\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function, gradient and Lipschitz constant\n",
    "A = train_images\n",
    "y = train_labels\n",
    "lam = 0.1\n",
    "\n",
    "beta = 0.25*np.linalg.norm(A, 2)**2 + lam\n",
    "\n",
    "def func(x):\n",
    "    z = A.dot(x)\n",
    "    return np.sum(np.log(1.0 + np.exp(z)) - y*z) + 0.5*lam*np.sum(x**2)\n",
    "\n",
    "def grad(x):\n",
    "    z = A.dot(x)\n",
    "    return A.T.dot(np.exp(z)/(1.0 + np.exp(z)) - y) + lam*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descend algorithm\n",
    "def optimizeWithGD(x0, func, grad, beta, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Optimize with Gradient Descent\n",
    "        min_x f(x)\n",
    "    where f is beta smooth.\n",
    "\n",
    "    input\n",
    "    -----\n",
    "    x0 : array_like\n",
    "        Starting point for the solver.\n",
    "    func : function\n",
    "        Input x and return the function value.\n",
    "    grad : function\n",
    "        Input x and return the gradient.\n",
    "    beta : float\n",
    "        beta smoothness constant\n",
    "    tol : float, optional\n",
    "        Gradient tolerance for terminating the solver.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iteration for terminating the solver.\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    x : array_like\n",
    "        Final solution\n",
    "    obj_his : array_like\n",
    "        Objective function value convergence history\n",
    "    err_his : array_like\n",
    "        Norm of gradient convergence history\n",
    "    exit_flag : int\n",
    "        0, norm of gradient below `tol`\n",
    "        1, exceed maximum number of iteration\n",
    "        2, others\n",
    "    \"\"\"\n",
    "    # initial information\n",
    "    x = np.copy(x0)\n",
    "    g = grad(x)\n",
    "    step_size = 1.0/beta\n",
    "    #\n",
    "    obj = func(x)\n",
    "    err = np.linalg.norm(g)\n",
    "    #\n",
    "    obj_his = np.zeros(max_iter + 1)\n",
    "    err_his = np.zeros(max_iter + 1)\n",
    "    #\n",
    "    obj_his[0] = obj\n",
    "    err_his[0] = err\n",
    "    \n",
    "    # start iterations\n",
    "    iter_count = 0\n",
    "    while err >= tol:\n",
    "        # gradient descent step\n",
    "        x -= step_size*g\n",
    "        #\n",
    "        # update function and gradient\n",
    "        g = grad(x)\n",
    "        #\n",
    "        obj = func(x)\n",
    "        err = np.linalg.norm(g)\n",
    "        #\n",
    "        iter_count += 1\n",
    "        obj_his[iter_count] = obj\n",
    "        err_his[iter_count] = err\n",
    "        #\n",
    "        # check if exceed maximum number of iteration\n",
    "        if iter_count >= max_iter:\n",
    "            print('gradient descent reach maximum number of iteration.')\n",
    "            return x, obj_his[:iter_count+1], err_his[:iter_count+1], 1\n",
    "    #\n",
    "    return x, obj_his[:iter_count+1], err_his[:iter_count+1], 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the algorithm\n",
    "x0 = np.zeros(image_size)\n",
    "\n",
    "x, obj_his, err_his, exit_flag = optimizeWithGD(x0, func, grad, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot convergence result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his)\n",
    "ax[1].set_title('norm of gradient')\n",
    "fig.suptitle('Gradient Descent on Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the identifier\n",
    "plt.imshow(x.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test result\n",
    "A_test = test_images\n",
    "y_test = test_labels\n",
    "\n",
    "z_test = A_test.dot(x)\n",
    "\n",
    "rate = np.sum(((z_test < 0.0) & (y_test == 0.0)) | ((z_test > 0.0) & (y_test == 1.0)))/num_test_images\n",
    "\n",
    "print('correctly classify %0.2f of the testing data.' % rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
